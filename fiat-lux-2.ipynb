{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of LLVIP images where size is too small: 23648\n",
      "LLVIP images: 18789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-01 09:31:24.874601: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: raw_data/oxford-iiit-pet/raw_data/images/grayscale/Egyptian_Mau_14.jpg; No such file or directory\n",
      "2024-04-01 09:31:24.874626: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: NOT_FOUND: raw_data/oxford-iiit-pet/raw_data/images/grayscale/Egyptian_Mau_14.jpg; No such file or directory\n",
      "2024-04-01 09:31:24.880952: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: raw_data/oxford-iiit-pet/raw_data/images/grayscale/Egyptian_Mau_156.jpg; No such file or directory\n",
      "2024-04-01 09:31:24.880959: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: NOT_FOUND: raw_data/oxford-iiit-pet/raw_data/images/grayscale/Egyptian_Mau_156.jpg; No such file or directory\n",
      "2024-04-01 09:31:24.937190: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: raw_data/oxford-iiit-pet/raw_data/images/grayscale/Egyptian_Mau_186.jpg; No such file or directory\n",
      "2024-04-01 09:31:24.937199: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: NOT_FOUND: raw_data/oxford-iiit-pet/raw_data/images/grayscale/Egyptian_Mau_186.jpg; No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file at raw_data/oxford-iiit-pet/raw_data/images/grayscale/Egyptian_Mau_14.jpg does not exist.\n",
      "The file at raw_data/oxford-iiit-pet/raw_data/images/grayscale/Egyptian_Mau_156.jpg does not exist.\n",
      "The file at raw_data/oxford-iiit-pet/raw_data/images/grayscale/Egyptian_Mau_186.jpg does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-01 09:31:26.107084: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: raw_data/oxford-iiit-pet/raw_data/images/grayscale/Abyssinian_5.jpg; No such file or directory\n",
      "2024-04-01 09:31:26.107098: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: NOT_FOUND: raw_data/oxford-iiit-pet/raw_data/images/grayscale/Abyssinian_5.jpg; No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file at raw_data/oxford-iiit-pet/raw_data/images/grayscale/Abyssinian_5.jpg does not exist.\n",
      "IIIT images: 7345\n",
      "Number of FLIR images where size is too small: 77034\n",
      "FLIR images: 3722\n",
      "Sampled LLVIP images: 3722\n",
      "Sampled IIIT images: 3722\n",
      "Sampled FLIR images: 3722\n",
      "Training images: 7816\n",
      "Validation images: 1675\n",
      "Test images: 1675\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import json\n",
    "\n",
    "# Function to parse XML annotation for bounding box\n",
    "def parse_annotation(annotation_path):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "    boxes = []\n",
    "    for obj in root.findall('object'):\n",
    "        bbox = obj.find('bndbox')\n",
    "        boxes.append((int(bbox.find('xmin').text),\n",
    "                      int(bbox.find('ymin').text),\n",
    "                      int(bbox.find('xmax').text),\n",
    "                      int(bbox.find('ymax').text)))\n",
    "    return boxes\n",
    "\n",
    "# Function to load and crop images based on bounding box\n",
    "def load_and_crop_image(image_path, box, small_size_counter, padding=50):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=1)\n",
    "\n",
    "    # Original image dimensions\n",
    "    image_shape = tf.shape(image)\n",
    "    image_height, image_width = image_shape[0], image_shape[1]\n",
    "\n",
    "    x1, y1, x2, y2 = box\n",
    "\n",
    "    # Check initial size of the bounding box\n",
    "    initial_width = x2 - x1\n",
    "    initial_height = y2 - y1\n",
    "\n",
    "    if initial_width < 100 or initial_height < 100:\n",
    "        small_size_counter['count'] += 1\n",
    "        return None\n",
    "\n",
    "    # Adjust box coordinates to include additional padding\n",
    "    x1_padded = tf.maximum(x1 - padding, 0)\n",
    "    y1_padded = tf.maximum(y1 - padding, 0)\n",
    "    x2_padded = tf.minimum(x2 + padding, image_width)\n",
    "    y2_padded = tf.minimum(y2 + padding, image_height)\n",
    "\n",
    "    # Compute new box dimensions\n",
    "    box_width = x2_padded - x1_padded\n",
    "    box_height = y2_padded - y1_padded\n",
    "\n",
    "    image_cropped = tf.image.crop_to_bounding_box(image, y1_padded, x1_padded, box_height, box_width)\n",
    "    image_resized = tf.image.resize(image_cropped, (224, 224))\n",
    "\n",
    "    return image_resized.numpy()  # Return the processed image as a numpy array\n",
    "\n",
    "# Function to prepare dataset for 'person' class\n",
    "def prepare_llvip_dataset(base_path, annotation_folder, image_folders):\n",
    "    images, labels = [], []\n",
    "    small_size_counter = {'count': 0}\n",
    "    \n",
    "    annotations_path = os.path.join(base_path, annotation_folder)\n",
    "    annotation_files = os.listdir(annotations_path)\n",
    "    for index, annotation_file in enumerate(annotation_files):\n",
    "        filename = annotation_file.replace('.xml', '.jpg')\n",
    "        annotation_path = os.path.join(annotations_path, annotation_file)\n",
    "        boxes = parse_annotation(annotation_path)\n",
    "\n",
    "        for folder in image_folders:\n",
    "            image_path = os.path.join(base_path, folder, filename)\n",
    "            if os.path.exists(image_path):\n",
    "                for box in boxes:\n",
    "                    cropped_image = load_and_crop_image(image_path, box, small_size_counter)\n",
    "                    if cropped_image is not None:\n",
    "                        images.append(cropped_image)\n",
    "                        labels.append(1)  # Label for 'person'\n",
    "                break\n",
    "\n",
    "    print(\"Number of LLVIP images where size is too small: \" + str(small_size_counter.get('count')))\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Function to prepare dataset for 'non-person' class\n",
    "def prepare_iiit_dataset(base_path, list_paths, image_folder):\n",
    "    images, labels = [], []\n",
    "\n",
    "    for list_path in list_paths:\n",
    "        with open(os.path.join(base_path, list_path), 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for index, line in enumerate(lines):\n",
    "                image_name, _, _, _ = line.split()\n",
    "                image_path = os.path.join(base_path, image_folder, image_name + '.jpg')\n",
    "                try:\n",
    "                    image_raw = tf.io.read_file(image_path)  # Read the raw image file\n",
    "                    image = tf.image.decode_jpeg(image_raw, channels=1)  # Decode JPEG image\n",
    "                    image = tf.image.resize(image, (224, 224))  # Resize for model compatibility\n",
    "                    images.append(image.numpy())  # Convert to numpy array for model compatibility\n",
    "                    labels.append(0)  # Label for 'non-person'\n",
    "                except tf.errors.NotFoundError:\n",
    "                    print(f\"The file at {image_path} does not exist.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {image_path}: {e}\")\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def prepare_flir_dataset(folders):\n",
    "    images_features = []\n",
    "    labels = []\n",
    "    small_size_counter = {'count': 0}\n",
    "\n",
    "    for folder in folders:\n",
    "        index_file_path = os.path.join(folder, 'index.json')\n",
    "        with open(index_file_path, 'r') as file:\n",
    "            index_data = json.load(file)\n",
    "\n",
    "        for frame in index_data['frames']:  \n",
    "            image_path = os.path.join(folder, 'data', 'video-' + frame['videoMetadata']['videoId'] + '-frame-' + str(frame['videoMetadata']['frameIndex']).zfill(6) + '-' + frame['datasetFrameId'] + '.jpg')\n",
    "            for annotation in frame['annotations']:\n",
    "                if annotation['labels'][0] == 'car':\n",
    "                    bounding_box = annotation['boundingBox']\n",
    "                    box = [bounding_box['x'], bounding_box['y'], bounding_box['x'] + bounding_box['w'], bounding_box['y'] + bounding_box['h']]\n",
    "                    image_feature = load_and_crop_image(image_path, box, small_size_counter)\n",
    "                    if image_feature is not None:\n",
    "                        images_features.append(image_feature)\n",
    "                        labels.append(0)  # All features belong to class '0'\n",
    "\n",
    "    print(\"Number of FLIR images where size is too small: \" + str(small_size_counter.get('count')))\n",
    "    return images_features, labels\n",
    "\n",
    "# Example usage\n",
    "with tf.device('/cpu:0'):\n",
    "\n",
    "    # LLVIP data labelled as 'person'\n",
    "    base_path_llvip = 'raw_data/llvip/raw_data'\n",
    "    image_folders_llvip = ['grayscale/train', 'grayscale/test']\n",
    "    annotation_folder_llvip = 'Annotations'\n",
    "    llvip_images, llvip_labels = prepare_llvip_dataset(base_path_llvip, annotation_folder_llvip, image_folders_llvip)\n",
    "    print('LLVIP images: ' + str(len(llvip_images)))\n",
    "    \n",
    "    # Oxford IIIT data (cats and pets) labelled as 'non-person'\n",
    "    base_path_iiit = 'raw_data/oxford-iiit-pet/raw_data'\n",
    "    list_paths_iiit = ['annotations/annotations/trainval.txt', 'annotations/annotations/test.txt']\n",
    "    image_folder_iiit = 'images/grayscale'\n",
    "    iiit_images, iiit_labels = prepare_iiit_dataset(base_path_iiit, list_paths_iiit, image_folder_iiit)\n",
    "    print('IIIT images: ' + str(len(iiit_images)))\n",
    "\n",
    "    # FLIR data (cars) labelled as 'non-person'\n",
    "    folders = ['raw_data/flir/raw_data/images_thermal_train', 'raw_data/flir/raw_data/images_thermal_val']\n",
    "    flir_images, flir_labels = prepare_flir_dataset(folders)\n",
    "    print('FLIR images: ' + str(len(flir_images)))\n",
    "\n",
    "\n",
    "    # Function to randomly sample images and labels from a dataset\n",
    "    def sample_images_and_labels(images, labels, sample_size):\n",
    "        # Generate random indices for sampling\n",
    "        indices = np.arange(images.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "        # Select the specified number of random samples\n",
    "        sampled_indices = indices[:sample_size]\n",
    "        return images[sampled_indices], labels[sampled_indices]\n",
    "\n",
    "    # Sample images from each dataset\n",
    "    sample_size = len(flir_images) # it's the smallest dataset\n",
    "    images_llvip_sample, labels_llvip_sample = sample_images_and_labels(llvip_images, llvip_labels, sample_size)\n",
    "    images_iiit_sample, labels_iiit_sample = sample_images_and_labels(iiit_images, iiit_labels, sample_size)\n",
    "    images_flir_sample, labels_flir_sample = flir_images, flir_labels # use all flir images\n",
    "    print(\"Sampled LLVIP images: \" + str(len(images_llvip_sample)))\n",
    "    print(\"Sampled IIIT images: \" + str(len(images_iiit_sample)))\n",
    "    print(\"Sampled FLIR images: \" + str(len(images_flir_sample)))\n",
    "    \n",
    "    # Combine the sampled data\n",
    "    images_combined = np.concatenate((images_llvip_sample, images_iiit_sample, images_flir_sample))\n",
    "    labels_combined = np.concatenate((labels_llvip_sample, labels_iiit_sample, labels_flir_sample))\n",
    "    \n",
    "    # Shuffle the combined dataset\n",
    "    indices = np.arange(images_combined.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    images_shuffled = images_combined[indices]\n",
    "    labels_shuffled = labels_combined[indices]\n",
    "    \n",
    "    # Split the data into training (70%), validation (15%), and test (15%) sets\n",
    "    train_ratio = 0.70\n",
    "    validation_ratio = 0.15\n",
    "    test_ratio = 0.15\n",
    "    \n",
    "    # First, split to get the training and the temp (validation+test) sets\n",
    "    X_train, images_temp, y_train, labels_temp = train_test_split(\n",
    "        images_shuffled, labels_shuffled, test_size=(1 - train_ratio), stratify=labels_shuffled\n",
    "    )\n",
    "    \n",
    "    # Then split the temp set into validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        images_temp, labels_temp, test_size=test_ratio/(test_ratio + validation_ratio), stratify=labels_temp\n",
    "    )\n",
    "\n",
    "    # Convert labels to categorical for model training\n",
    "    y_train = to_categorical(y_train, num_classes=2)\n",
    "    y_val = to_categorical(y_val, num_classes=2)\n",
    "    y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "    # At this point, X_train, X_val, X_test, y_train, y_val, and y_test are ready for training, validating, and testing your model.\n",
    "    print(\"Training images: \" + str(len(X_train)))\n",
    "    print(\"Validation images: \" + str(len(X_val)))\n",
    "    print(\"Test images: \" + str(len(X_test)))\n",
    "    \n",
    "    # File paths\n",
    "    train_images_file = 'data/train_images.npy'\n",
    "    val_images_file = 'data/val_images.npy'\n",
    "    test_images_file = 'data/test_images.npy'\n",
    "    train_labels_file = 'data/train_labels.pkl'\n",
    "    val_labels_file = 'data/val_labels.pkl'\n",
    "    test_labels_file = 'data/test_labels.pkl'\n",
    "\n",
    "    for file in [train_images_file, val_images_file, test_images_file, train_labels_file, val_labels_file, test_labels_file]:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "    \n",
    "    np.save(train_images_file, X_train)\n",
    "    np.save(val_images_file, X_val)\n",
    "    np.save(test_images_file, X_test)\n",
    "    with open(train_labels_file, 'wb') as f:\n",
    "        pickle.dump(y_train, f)\n",
    "    with open(val_labels_file, 'wb') as f:\n",
    "        pickle.dump(y_val, f)\n",
    "    with open(test_labels_file, 'wb') as f:\n",
    "        pickle.dump(y_test, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T07:33:32.897006Z",
     "start_time": "2024-04-01T07:29:38.647013Z"
    }
   },
   "id": "da907ab8a677a5b7",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train = np.load(train_images_file)\n",
    "X_val = np.load(val_images_file)\n",
    "X_test = np.load(test_images_file)\n",
    "with open(train_labels_file, 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open(val_labels_file, 'rb') as f:\n",
    "    y_val = pickle.load(f)\n",
    "with open(test_labels_file, 'rb') as f:\n",
    "    y_test = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec8750ace868b7cf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import MaxPooling2D, Flatten, Dense, Convolution2D, Activation, Input, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(224, 224, 1)))\n",
    "model.add(Convolution2D(32,(3,3),padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Convolution2D(32,(3,3),padding='same'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#model.add(Convolution2D(32,(3,3),padding='same'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "#model.add(Convolution2D(32,(3,3),padding='same'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(40))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# compile model and initialize weights\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64340eee14519402",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fc484146292317",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "    batch_size=32,  # Adjust based on your dataset size and memory constraints\n",
    "    epochs=50,  # Adjust based on the desired number of training epochs\n",
    "    validation_data=(X_val, y_val)) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51daf7b919bf0860",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(history.history)\n",
    "\n",
    "# Corrected plotting code\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.subplot(1, 2, 1)  # Corrected subplot call\n",
    "plt.plot(history.history['accuracy'], linestyle='-.')\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)  # Corrected subplot call\n",
    "plt.plot(history.history['loss'], linestyle='-.')\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b94de3def8efd16",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33f3269e1efe2dc1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a72f29c461574135"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
